{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xfx2e4xoo59"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from transformers import EarlyStoppingCallback\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnyBheMJgvgD"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"author_profiling_v3.csv\")\n",
        "\n",
        "# # Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Preprocess tweets: tokenize and pad/truncate\n",
        "max_length = 64\n",
        "df['tweets'] = df['tweets'] .astype('str')\n",
        "df['tweets'] = df['tweets'].apply(lambda x: tokenizer.encode_plus(x, truncation=True, padding='max_length', max_length=max_length))\n",
        "\n",
        "\n",
        "# Drop original 'gender' and 'age_group' columns\n",
        "df.drop(['gender', 'age_group','cleaned_tweets','tweets_lemmatized','tweets_withouthashtag'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Split into train and test first\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split df_train into train and validation\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a PyTorch dataset\n",
        "class PersonalityDataset(Dataset):\n",
        "    def __init__(self, tweets, targets):\n",
        "        self.tweets = tweets\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = torch.tensor(self.tweets[idx]['input_ids'], dtype=torch.long)\n",
        "        attention_mask = torch.tensor(self.tweets[idx]['attention_mask'], dtype=torch.long)\n",
        "        targets = torch.tensor(self.targets[idx], dtype=torch.float)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'targets': targets\n",
        "        }\n",
        "\n",
        "class RoBERTaForPersonalityTraits(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RoBERTaForPersonalityTraits, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 5)  # Update the output size to 5\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        output = self.dropout(pooled_output)\n",
        "        output = self.linear(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "BJxXEFNDkTg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data loaders\n",
        "batch_size = 16\n",
        "train_dataset = PersonalityDataset(df_train['tweets'].tolist(), df_train[['ext', 'neu', 'agr', 'con', 'ope']].values)\n",
        "val_dataset = PersonalityDataset(df_val['tweets'].tolist(), df_val[['ext', 'neu', 'agr', 'con', 'ope']].values)\n",
        "test_dataset = PersonalityDataset(df_test['tweets'].tolist(), df_test[['ext', 'neu', 'agr', 'con', 'ope']].values)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Instantiate the model and define optimizer and loss function\n",
        "# model = Longformer()\n",
        "model = RoBERTaForPersonalityTraits()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "9UuWAYFElmw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390ff92f-1802-4e23-d9e8-e0994dc11d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 10\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "accumulation_steps = 28\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad() if i % accumulation_steps == 0 else None\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "        if (i+1) % accumulation_steps == 0 or i+1 == len(train_loader):  # update on last step even if accumulation steps have not been reached\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()  # make sure to reset gradient after updating\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Delete unnecessary tensors to save GPU memory\n",
        "        del input_ids\n",
        "        del attention_mask\n",
        "        del targets\n",
        "        del outputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Train loss at epoch {epoch + 1}: {avg_train_loss}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_total_loss = 0\n",
        "    all_predictions = [[] for _ in range(5)]  # Separate predictions for each trait\n",
        "    all_targets = [[] for _ in range(5)]  # Separate targets for each trait\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            val_total_loss += loss.item()\n",
        "\n",
        "            # Split predictions and targets for each trait\n",
        "            for i in range(5):\n",
        "                all_predictions[i].extend(outputs[:, i].cpu().numpy())\n",
        "                all_targets[i].extend(targets[:, i].cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_total_loss / len(val_loader)\n",
        "    print(f\"Validation loss at epoch {epoch + 1}: {avg_val_loss}\")\n",
        "\n",
        "    # Check if early stopping conditions are met\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0  # reset counter\n",
        "        # save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1  # increment counter\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break  # break out of the training loop\n",
        "\n"
      ],
      "metadata": {
        "id": "WdAXlvoTnvR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ed4015-eff5-4ba6-c0e4-db31eb7792ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss at epoch 1: 0.0015698678076715264\n",
            "Validation loss at epoch 1: 0.028544730094092138\n",
            "Train loss at epoch 2: 0.001416440096620889\n",
            "Validation loss at epoch 2: 0.028648711486973545\n",
            "Train loss at epoch 3: 0.0013196915678410658\n",
            "Validation loss at epoch 3: 0.02798172051552683\n",
            "Train loss at epoch 4: 0.0012523203113967685\n",
            "Validation loss at epoch 4: 0.028014787927862595\n",
            "Train loss at epoch 5: 0.0012065046729997384\n",
            "Validation loss at epoch 5: 0.0275388421617787\n",
            "Train loss at epoch 6: 0.0011604977192502996\n",
            "Validation loss at epoch 6: 0.02686394271652468\n",
            "Train loss at epoch 7: 0.0011255693604441763\n",
            "Validation loss at epoch 7: 0.026304453621957113\n",
            "Train loss at epoch 8: 0.0010888563557466223\n",
            "Validation loss at epoch 8: 0.02558827465823428\n",
            "Train loss at epoch 9: 0.0010726135460756285\n",
            "Validation loss at epoch 9: 0.02536240938669917\n",
            "Train loss at epoch 10: 0.0010452240475378766\n",
            "Validation loss at epoch 10: 0.025414319698360156\n",
            "For Extroversion:\n",
            "MSE: 0.023549817502498627\n",
            "RMSE: 0.15345950424671173\n",
            "R^2: 0.11413572837021191\n",
            "\n",
            "For Neuroticism:\n",
            "MSE: 0.04040978103876114\n",
            "RMSE: 0.20102183520793915\n",
            "R^2: 0.2327060897449209\n",
            "\n",
            "For Agreeableness:\n",
            "MSE: 0.021998072043061256\n",
            "RMSE: 0.14831747114658356\n",
            "R^2: 0.04066961723340401\n",
            "\n",
            "For Conscientiousness:\n",
            "MSE: 0.021466337144374847\n",
            "RMSE: 0.1465139538049698\n",
            "R^2: 0.06014816853312521\n",
            "\n",
            "For Openness:\n",
            "MSE: 0.019647587090730667\n",
            "RMSE: 0.14016984403133392\n",
            "R^2: 0.1613445227022129\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test set\n",
        "model.eval()\n",
        "test_total_loss = 0\n",
        "all_predictions = [[] for _ in range(5)]  # Separate predictions for each trait\n",
        "all_targets = [[] for _ in range(5)]  # Separate targets for each trait\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        test_total_loss += loss.item()\n",
        "\n",
        "        # Split predictions and targets for each trait\n",
        "        for i in range(5):\n",
        "            all_predictions[i].extend(outputs[:, i].cpu().numpy())\n",
        "            all_targets[i].extend(targets[:, i].cpu().numpy())\n",
        "\n",
        "trait_names = [\"Extroversion\", \"Neuroticism\", \"Agreeableness\", \"Conscientiousness\", \"Openness\"]\n",
        "\n",
        "for i in range(5):\n",
        "    predictions = all_predictions[i]\n",
        "    targets = all_targets[i]\n",
        "\n",
        "    mse = mean_squared_error(targets, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(targets, predictions)\n",
        "    r2 = r2_score(targets, predictions)\n",
        "\n",
        "    print(f\"For {trait_names[i]}:\")\n",
        "    print(f\"MSE: {mse}\")\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "    print(f\"MAE: {mae}\")\n",
        "    print(f\"R^2: {r2}\\n\")"
      ],
      "metadata": {
        "id": "peS5ZbzR_wI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f4c26e-13cf-4c48-a7f4-1de1ae80999f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Extroversion:\n",
            "MSE: 0.022939695045351982\n",
            "RMSE: 0.15145856142044067\n",
            "MAE: 0.11859782040119171\n",
            "R^2: 0.12933367100562165\n",
            "\n",
            "For Neuroticism:\n",
            "MSE: 0.039613835513591766\n",
            "RMSE: 0.1990322470664978\n",
            "MAE: 0.16198262572288513\n",
            "R^2: 0.22804763320969812\n",
            "\n",
            "For Agreeableness:\n",
            "MSE: 0.02363361231982708\n",
            "RMSE: 0.1537322700023651\n",
            "MAE: 0.11621047556400299\n",
            "R^2: 0.018035479145879107\n",
            "\n",
            "For Conscientiousness:\n",
            "MSE: 0.02075071819126606\n",
            "RMSE: 0.14405108988285065\n",
            "MAE: 0.1176556795835495\n",
            "R^2: 0.07265784955979493\n",
            "\n",
            "For Openness:\n",
            "MSE: 0.01933954283595085\n",
            "RMSE: 0.139066681265831\n",
            "MAE: 0.11536608636379242\n",
            "R^2: 0.18549295347818118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions = pd.DataFrame(np.transpose(all_predictions), columns=trait_names)\n",
        "correlation_matrix_predictions = df_predictions.corr()\n",
        "print(\"\\nCorrelation matrix for predicted values:\")\n",
        "print(correlation_matrix_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1KMDsl2CMnY",
        "outputId": "8480ce1d-ffd7-4332-cace-e1b8015bb070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correlation matrix for predicted values:\n",
            "                   Extroversion  Neuroticism  Agreeableness  \\\n",
            "Extroversion           1.000000     0.377298       0.338042   \n",
            "Neuroticism            0.377298     1.000000       0.563185   \n",
            "Agreeableness          0.338042     0.563185       1.000000   \n",
            "Conscientiousness      0.334508    -0.072858       0.426295   \n",
            "Openness              -0.056672    -0.286838      -0.424843   \n",
            "\n",
            "                   Conscientiousness  Openness  \n",
            "Extroversion                0.334508 -0.056672  \n",
            "Neuroticism                -0.072858 -0.286838  \n",
            "Agreeableness               0.426295 -0.424843  \n",
            "Conscientiousness           1.000000  0.086586  \n",
            "Openness                    0.086586  1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df[['ext', 'neu', 'agr', 'con', 'ope']].corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkyKtZ9TD4Yk",
        "outputId": "aa7e667b-ee11-48b2-f03c-81d03a6a30d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ext       neu       agr       con       ope\n",
            "ext  1.000000  0.294476  0.145334  0.192219  0.020805\n",
            "neu  0.294476  1.000000  0.325530  0.021377 -0.029465\n",
            "agr  0.145334  0.325530  1.000000  0.070499 -0.004108\n",
            "con  0.192219  0.021377  0.070499  1.000000  0.071473\n",
            "ope  0.020805 -0.029465 -0.004108  0.071473  1.000000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}